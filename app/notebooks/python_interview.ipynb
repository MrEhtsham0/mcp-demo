{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691304b6",
   "metadata": {},
   "source": [
    "## **Shallow Copy**\n",
    "\n",
    "1=>By changing one, It also affects on other variable,or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4f056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1, 2, [3, 4, 5]]\n",
      "b: [1, 2, [3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "a = [1, 2, [3, 4]]\n",
    "b=copy.copy(a)\n",
    "b[2].append(5)\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1, 2, [3, 4, 5, 6]]\n",
      "b: [1, 2, [3, 4, 5, 6, 1217]]\n"
     ]
    }
   ],
   "source": [
    "# Deep COPY\n",
    "a = [1, 2, [3, 4,5,6]]\n",
    "b=copy.deepcopy(a)\n",
    "b[2].append(1217)\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956d3a9",
   "metadata": {},
   "source": [
    "**Difference between is and ==?**\n",
    "\n",
    "==? means value equal\n",
    "is? means same memory location\n",
    "\n",
    "**What is StaticMethods and class methods**\n",
    "1=>Does NOT take self (instance) or cls (class) and Cannot access instance variables or class variables.\n",
    "2=>classmethods take **cls** orguments, It can change the class states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bffa42c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "ABC Corp\n",
      "XYZ Inc\n"
     ]
    }
   ],
   "source": [
    "class MathOps:\n",
    "    company=\"ABC Corp\"\n",
    "    @staticmethod\n",
    "    def add(a, b):\n",
    "        return a + b\n",
    "    @classmethod\n",
    "    def change_company(cls, new_name):\n",
    "        cls.company = new_name\n",
    "\n",
    "print(MathOps.add(5, 3))   # 8\n",
    "print(MathOps.company)     # ABC Corp\n",
    "MathOps.change_company(\"XYZ Inc\")\n",
    "print(MathOps.company)     # XYZ Inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141a6d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "## Dict comprehension\n",
    "my_list=[2,3,4,5,6,7,8,9,10]\n",
    "squares={x:x**2 for x in range(10)}\n",
    "lambda_square=list(map(lambda x:x**2,my_list))\n",
    "print(lambda_square)\n",
    "# print(squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbe274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there!\n"
     ]
    }
   ],
   "source": [
    "# Monkey patching example\n",
    "class Person:\n",
    "    def greet(self):\n",
    "        return \"Hello!\"\n",
    "def new_greet(self):\n",
    "    return \"Hi there!\"\n",
    "Person.greet = new_greet\n",
    "p = Person()\n",
    "print(p.greet())  # Hi there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96feefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7592a9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# import os\n",
    "load_dotenv()\n",
    "\n",
    "# HUGGINGFACE_ACCESS_TOKEN=os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\")\n",
    "# print(HUGGINGFACE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d2825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceEndpoint(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "model = HuggingFaceEndpoint(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "response = model.invoke(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077d941",
   "metadata": {},
   "source": [
    "# **How Planner works in LangGraph**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7f0e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab26dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the script.\n"
     ]
    }
   ],
   "source": [
    "print(\"End of the script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f8e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13a95c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97886/2518516040.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tools = [TavilySearchResults(max_results=3)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4278c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\",api_key=OPENAI_API_KEY)\n",
    "prompt = \"You are a helpful assistant.\"\n",
    "agent_executor = create_agent(llm, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2518e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=agent_executor.invoke({\"messages\": [(\"user\", \"Who is the president of the United States?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec4c2205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The President of the United States is Joe Biden. He assumed office on January 20, 2021.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].content\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class PlanExecuteState(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45224427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec447c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import Image, display,Markdown\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objective or user input query,You have to behave like planner engineer that breaks the bigger problem into smaller one and come up with a simple step by step plan.For any problem scenario, you need to make step by step and bullet points plan to solve that problems.The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41af1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_node(state: PlanExecuteState):\n",
    "    \"\"\"Generates a plan from user input.\"\"\"\n",
    "    user_input = state[\"input\"]\n",
    "    \n",
    "    planner = planner_prompt | ChatOpenAI(\n",
    "        model=\"gpt-4o\", temperature=0,api_key=OPENAI_API_KEY\n",
    "    ).with_structured_output(Plan)\n",
    "    \n",
    "    planner_response=planner.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"user\", user_input)\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    display(Markdown(\"\\n\\n\".join(f\"{i+1}. {s}\" for i,s in enumerate(planner_response.steps))))\n",
    "    \n",
    "    return {\"plan\": planner_response.steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6096baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given list of planner{plan},You have to behave like executor the plan like developer those converts the requirements into code.The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{plan}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "561b4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_plan(state: PlanExecuteState):\n",
    "    plan = state[\"plan\"]\n",
    "    executor_agent=executor_prompt | ChatOpenAI(\n",
    "        model=\"gpt-4o\", temperature=0,api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    final_response = executor_agent.invoke({\"plan\": plan})\n",
    "    print(final_response)\n",
    "    return {\"response\": final_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9393a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START,END\n",
    "\n",
    "workflow = StateGraph(PlanExecuteState)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "# Add the execution step\n",
    "workflow.add_node(\"agent\", generate_response_from_plan)\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "workflow.add_edge(\"agent\", END)\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cec5ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAAFNCAIAAABnnW36AAAQAElEQVR4nOydB2AUxf7HZ3fvLr33QhotNBMwIDyaEtrjL4QAAtJUlCZNISBPilJ8qIiiIiAq6kMjTxEfRUQUBExoobeApJGEkIT03CW5tvufvU0u7e72bicXl9x8jMfezOzu3fem/HbaT8IwDMAIRQIwCGD5kMDyIYHlQwLLhwSWDwlU+TJvVqddlpcXK6sVWloDAANICYAHBAlomqEkBHwFNMG+ZRiS0L2FkfAt/Af+T8JrMAQg4D8wCh4QFGC0MJw9hjYVtKu4WPaF0AXS7MVZe4upDSeI2nP1yeAddcdsGkICGE2jzyx1IGV2pKOLJLSLU7d+zgABQpjdd+lYxc2zZZVlaoZmZHaU1I4gKVYjRsuQUDINQ5Ds94RSQp3Y7wbf6r4aezeoCEVAoRkunJWEuyrNQKEpAl4EysrKAdMA7hwWeAum7ixWPy6YIFj54DXr3rI/iVb3hmQjSClBqxt9R6kdpdUw6hpaqdTSNLB3pMK7OT010QdYjsXyXTpWduH3EnhX32CHmKFeIV1k4FGmsoRJ2l9wP71ao6bDe7iMmOZr0emWyfef9VlVcrprX7dB8V6gbZF6XnH60ENYUGZtCDf/LAvk274s3aed/YRFQaDtcnJv0Y0zZf2f9ol+ys2c9ObKt3VJ2pCJ/l37IlW0jwrbEtKm/ivczYviTWmWfJ8sTZu1oYPMAdgOn67IiIn1fHyYu+lkJOBjx2vpsZP8bUo7yJy3I84dLS4rpE0n45Hv63X3fILtI/vYRJltQt8R3nvezzKdxpR8F38vq1Joxy9sy22FCXoNdbN3JH/4MNdEGpPyHS/t0Zen8LdtJiwKKbhXYyKBUfmunqjQqukB8Z7AhnF2J51dpfu25hlLYFS+y6dKfdq1dnsxbNiw+/fvAwtJT09/+umngXV4bKB7fna1sVij8lVVavoM9watyIMHD0pLS4Hl3Lp1C1iNXrFu8Bk8+7bhImy4x+XuFQU0CUMirfI8Cy3N77777tChQ/fu3QsPD+/bt++8efMuX748d+5cGBsXFzd48ODNmzfDPLV3796UlJS8vLyIiIixY8dOmDCBu0JsbOxLL710/PhxeNb06dN3794NA2NiYl599dWpU6eClsbeWXIjuTwk0r55lGH5Mm8opHbASuzZs2fXrl2vvPJK//79T5w48cknnzg5Ob3wwgtbtmyBgfv37w8KYtt6qCAUbuXKlbBHJSsr65133gkICICnwCipVPrTTz/16dMHivj444/DBEePHoW/B7AOLm6SkkKlwSjD8lUUq2E3DrAOly5d6tq1K1dbxcfH9+7du6qqqnmyjRs3KhSKwMBAoMtZBw4cOH36NCcf1MvNzS0hIQG0Cm7esvtpVQajDMunVmplMv4HEmFERUV9/PHH69at69mz56BBg4KDgw0mg2Uc5tPk5GRYxrkQLldywB8AtBb2TqRabfjxw7B8Wi3N9ghbhylTpsDSevLkybVr10okEtjaLlq0yMenUW8lTdOLFy9WqVQLFiyAWc/FxeXFF19smEAma71+RtgNTBCGowzLJ7OTKKu1wDqQJBmvIyMj4/z58zt37pTL5R988EHDNLdv37558+a2bdtgBceFVFZW+vpa1pfZUlRX0iRpWD/D8rl4SCtK1MA6wDq+S5cu7du3j9ABdYHtQJM0ZWVl8FWvV4YOeAr4O5CXqe0cDQtluIS26+yoquHpbBDMkSNHli1bdurUqfLy8qSkJGh/wNoQhoeFhcHX33777caNG1BWWK6hRVJRUQGb3U2bNkH7BhqGBi8YEhJSVFQEG3F9LdmylBaqnF0NN6SG5ev+DxdayxQ/UAErsGrVKqjOkiVLoPm2fv16aOVB6wSGwzZk9OjRO3bsgA2Lv7//hg0brl+/PmTIEGjNzZ8/Hxp9UFa96deQAQMGREdHw4b4119/BVagplrbtY+rwSij3aU7V2b4tbOPmxsIbJvb5+XHvs+f/14Hg7FGm9fIGFdjxo5NceZwkaun1Fis0WHyQfHeN5LLLp8o7/mk4UGT/Pz8yZMnG4xydnaGjanBKFhs4SMHsA5f6TAYBS1tY+UM2kYG6wQOebl6zr87GIs1NdZx7LvCu1fkc9+JMBir0WgKCwsNRtXU1Njb2xuMgg2C9eyPSh0Go2AT5OpquP6C4fD3Nhj1zVv34GD/9NWhwAg8Q0U7X88IjXQaMcMP2B7Zd2oO7sydv7mDiTQ8jxaz/x2RdlVeU24tI0bMHP4ib8BYnoLC/2Q2bIrfl29lAhtj15tZwZ0cowa6mk5m1jhvSb4qcVPOgs1/j9Hf+mxfnj54vF/XJ/jHF82dZZB5s/rnz+9HD/YYMLatzW5pSHZq9eGv8kI6O42a6W9OekumCNFg56pMqYwYMT0gsL3VelP/PhLfzSl/qBoY7wsfusw8xeIJaod35d9LVdg7Ux0ecx4Y36qDIVbiysnKG6dLy4vVXv52kxOCLTpX4PTIw18W5KYpVNU0zIz2ThTsobFzoBh4MW391dgppLRuvqJ+fiMXwt2YZBhdpyLdoFXXTapk2GmWDQIpitDqLtv49AZp2GmXXIjuqP5yups3sxooCaVV0ZVlmmqFFvbLkRThE2A3fl4QsLwLUaB8HPIS7bmjJQXZ1coqWqOGXZwE3UA+TrfalwYh+juzM2cbhhgKhBeVUAStm4fbODE3WxU0vB3g5vI2CmwaAnS/h9SOsLOnPPykPfp7BHcSXhEhydcKjBgxIjEx0ctLpO2V2GfWw0dD+JwHxAqWDwksHxJil0+tVsNBcSBWRC0frbNTSKsNmaIjavlEXnIBlg8RUX84kVd8AOc+RLB8SGD5kMDyISF2+XDTIRyc+5DA8iGB5UMCms1YPuHg3IcElg8JLB8SWD4kcI8LEjj3IUFRlIuLudNN/hbEPlRUXl4ORIy4i4ZEAssvEDFYPiSwfEhg+ZDA8iEhdsMFyyccnPuQwPIhgeVDAsuHBJYPCSwfElg+JLB8SGD5kBC/fGJcVbR27doDBw5wH4xdU6WDJMmUlBQgMsQ4aX3evHlhYWGkDvjYC1+hfMY2Wvt7EaN8vr6+Q4cObRgC5YuLiwPiQ6RLJqZNmxYaWr/9R1BQ0NixY4H4EKl8cIBt9OjR+gUxw4cPd3cX4w7S4l2wM2XKFK6+CwwMHDduHBAllrW8yftL5BUqbm86SkJoNey5Os9BuqXbugXKNF2/1JtbwAyrfrpujTi3Irw2nGKXT3MryOvS1K4mh8DE9/Pup6WlBQQGdOrQCdRtpNTgvs2WnpP1n6H+G5K6JeXc56lLzN2uybeTSElHV2mf4V4OZm/Rb658h78suJeqkEhYh0RqpW5pPMXQ2jqPSrq13AxBE4BkPx9R64eJIdj/OJkaLJnXZXooJQVobe0xF06QNEPrNWAdPsETYdtbJ5DOTZTOlZTuvg0uWKsUZ+c0XoCvu15tSN1i/VqHUo2hpPCjAnUN4+Yjm7LcrIbeLPmSDhTfOlsxek64s23sYP/T1lx7BzBxCb+C/PIdTyzKTFVMTAgFtsShT3OhNs8ua2c6GX/TkXZD3rWvzW38//Sc4LJCFeDb/pZHvmo50Ki03QeIep6OlZDIyOTDJTxpTEfLy1XNWygbQaulFRU8u7fyyEerAbBV99FabSMDyCDYxadRCJ1DR9NpsHxGYe1tvpaVRz6GsNmyy24fRRjbq74OHvlgBua7QpuFZvjrfVx4TcEAtNwH7ULeS7RZWqDpoHVOhm0S2K1ASXjaDj75bDXnQWCXjFbDY/jxyWerzS4LAQBiy1vrHdx2Qaz7CNstvtBspvh2H+Mzq1tUvDfXvpaw7GXwiMBoGdhrYDoN31MHsF3DBT5xoT60EYwNGy6MgU2Lm9DyA5Xf//DN2HFDk5JOjJswfMjQ3tNmxB89+nPzZJmZ6R9+9M5zL0wY8c9/zJk7bf+Bvfrwp2JjUm/fXL0mAR5MnDxq+44tWq3WdBTk5s1ry19bMCbuqenPjdu2/QOFQsGF/7hvz/hnRiQln4gd1ufq1UvAbFi7j0Ks+ywvvBQlUSjkx44f+Xb3/v/9dCx2yIi3330zJ6epD6tPtm1OSTmzeNFrb2/8aNSosVDKs+eSgc7/JHzd/P6G2NiRR4+cWfmvDfD3+OPEb6ajcu/nJCx/uUZZs/XjL9evfS8j4+6rS2Zzs7NkMllVleLAgb3/WrEuon1HYDYw6/F2FfPLJ6Dwws89Ln6yg4ODq4vr88/NcXJ0Ona8qQ+r1as3btq0rVfP3j2jY+LGTOjcqcv5lNP62MGDhj45eCjUKyqqV2BA0F9/pZqO+v33X6QSKRQuJCQsLCwiYenqu2l3YI4DutHhmpqayZOfGxo70sXZklEHBjCIXQa6ulNI09GpU5faKxBEYGBwdnZm0xQMs2/fnnPnk/UZMyAgqPnpgPV85CKXV5qOunnzamRkNze32oFUf/8AeNNr1y9DobmQyM7dgKUQvP1VvJ31Qs1mO7v6nczt7O1hcW50WZpe8fpitVo166UF0dExMFMsXNzIBaWJ7V4NRkERb9+5BSvEhoGlJcX6YyE+LRn+MXBrtbyw5nZycuKOlTU1Hu6Nhjr/uss6oXxv07bHe9W6oITf38dbuAsoTy/vHj2iX3h+bsNAN1e0UX0KGHNNqYffbG6+Zb45XL5SOxNUqVRm52SFhzfyNFNezrqg1OuVlZUB/wAC7SM6FhbmRz3WC9ak3B/8wWA9CFDQojcdbG+zxbkPli9Yr2VnZ0GrYteX26GCsUNGNkwQFsq6oPzv97srKitgso+3buod0ze/4AEQyoQJU2GFsHXbZthKwMr0050fzXxpUkZmGrAyVnlog1XuxGemLUmYO3T4EwcP/bhi+Zvt2jWa4+Hn57/y9Q23Uq/HjR3y+qpXX3px/pgxE1JTb0AzEAgCNvFffP5fB3uHOfOmzXh+/JWrF5clrO7UMRKgQBK8hZdnjkthlur7D7Ofe7MDMBtopm7b/v6x386DR5zd69PDezj+87kAE2n4n3ltt8+PBCQeqBSMbl0EWsur63C1rP4bP25yGyi5EAI+tCE+dbC5z8b7m03CZzbTtjxYxA9/Z73NCkhQDEUiDhXZ8FgHoyW0eJjcqpgxx4XBtZ9RzJhlQNjqWIcESCjESRo2PM+A0QAN4kCl7jK48BqFf5IGYauF1xz45KMYkrLR3CdzoGR2lOk0PFWjXzs7OFxUXmyLGVCrpoM78Kyt5B+odHaTpBzOBzbG7ZQKOMrYOcbRdDJ++WasCinIrs5LE/WGIC3OhaPF/Ub58SYzdz3vpysynN2kIZHOLl4SjcbAKYRxE6dJFMM3BEDUpTF6QUO9kPr0JDvIU3uL5heBnSAMafiTwCdcZRWTc7eyKE81LSHExYen4gPmywfZu+V+6UO1Rq3VqA2cQkmAtlkGrV01ThANO84afiXO+XWTL8m91XsibxqrWyzNNIuql0+3ut2YvgyrXq0x0eTKcGRDlS+CugAADu1JREFUKiWd3CVPP9/OzR+Yg9ida48cOfLbb7/FzrUFgt0bI4HlQ0Lk3p5w7kNC1PLBZo2maYriNyD+LrC3GCSwfEhgV09I4NyHBJYPCSwfErjuQwLnPiSwfEhg+ZDA8iGB5UMCy4cElg8JLB8S2GxGAuc+JLB8SIjdW4yPjw8QMaKWT6vVFhYWAhGDfRUhgeVDAsuHBJYPCSwfElg+JMQun36HL3GCcx8SWD4kxC4f7HQBIgbnPiSwfEhg+ZDA8iGB5UMCy4eEGFcVLVy4MCkpSb/5E0mSNE3DtxcvXgQiQ4wOZhcvXhwcHEzWAXQKhoSEAPEhRvk6dOgwYMCAhsUCZr3BgwcD8SFe59rt2tW714THEyYI3FjSqohUvqCgoNjYWO4YVnwxMTGcp2ixIV7n2pMnT+a8u8PXSZMmAVHSAoZL5rXq6ip1k82y6lca648MriKv8yPdKEy3RhoAu+H9Zv1RfbxH5x7VhT43CisIY1tZGll4znqMMG5WyGTSTo87ADSQDJeftj7Iz66G30qjbr7PIt0ka+t0aq6fAVHr5GuaoHF4g/RGlu0bXKSuR2pHMlrg4iGd9jqPD2MTCJdv39YH5UWqweMCfEIt389cHKhqwPE9DyofKmduCAOCEChf4ru5sBd97MtirM4t5czBkpw7FS+uDwOWI6TpKHkAyh8q24Z2kH6jPWEeOvVjiYBzhch34ehDO0exr6O2CDdv++y/5JafJ0g+eaUKtC1IKaOsFjKkJyQTaVQ0/ANtCGg5aASNqbSpMigYwfuzYvlYWD/agrbZEyIfvBkg21ThFYwQ+Vinv7R4H5ZbE1x4WRhaoGMIIfKREoIU79YqwmjFpgMW3rbmxIPk21PQCMLk4/d9+YhBC3Trgus+FhIaLqSQ7CfIcGlzO4nD8QBh5UlQ4W1zTiha12xmXS/indhZhFi/OvHEW37Xrltx+Jf9Fp3Cbgkr6AsJenhgRO197M6dW8BSaHF3GWRmph84uPfS5ZT8/Lyw0IhRo8bGjakd9i4tLdn49pqbt66FtAuLi3smNzf7z6Q/vv6S9bWt0Wi+2LXt7LmkwsL87t2j4+Mm9u07gLvazJcmbfvk68TEL5OST/j4+D715PDZsxZSFMX5mN303vrtOz44uP8EsDJCch8BLDYyjbnShrz73rrsnKxN727bsP79c+eS4Z/eA+9HH7+798fE+LGTEr89OHhQ7Btrl588dQyY9LJ95DB72WUJqy3TTmjTIazus/ghx5gr7fLysrNnkyY+M71rl+5eXt5Ll6yC2ZM7RalU/nr00JRnnx8zerybq9uof8bFDhn5n92f6a9pwgG3pRBCn9oEP/Na+GMZcaWdnnEXvnbvHsUFOjs79+rVB2ZGeAzlUKlUvWP66a8RHfX4L0cOlFeUc29NOOAWgDBLQoh8tIahtRb8WiZcaVdWVsBXJ6d6vw6urm7cASdHE6fbQOcym1uhT5It1mkm+Cm+NZqOjIw0Y6607ezs4ataVT/2VFpWO2Do5c0uw1+6ZGVQUKNZAL6+/iUlRUAcCJKPBIwlHngq5WwWa+JKOzyMdbfNuT3OzEoPC4sArKzyS5fO+/mxLnGDg0I4//CwuuROhG00zCSOjo4lQoZkTUG0ZtMBLHQe2C441Jgr7aDA4NDQ8K//s/N+Xi7UbsuHG7k6EQJlev65ObCtuH79CqwEYZubsPzlLR++bfpeUHFox1y4cPbylQvAbAR3lwqRD/aVUpac5+3tY8KV9vKENbAWmz4j/tUls2Fr0L1blFRSu/XN5EkzliWsSdzz1ei4J6GtExgQvHTpKt7bTZ0yExqYq9cstaQ+E2g2C5nj8v37OWUPNc+uCActAbRdampq/PxqHWT8a+UrEkqyft17oBU58nVuab569r8t/kaCCi+s+MgWe2qDj6gw38EnDajj7m++uHjx3JgxrT0Pt3VbXoZgu2dbiDfeeGfTe+s++3zrw4cFoSHhb6x+G9aM4BFBiHwSGZC03MZc8Iliw7rN4G+FpEhhg1/C5rgAjahX2VoMraVpQYv+hc0yaMm6TxSwPcCtNVTE0C1Z94kCgiGJ1hoqoiSAamOOK1tzloFWA7TaNlV4W3moCN4PtCVa1e5jffC1rVkGrZr72p7D8tbu78PDvBzCHtp0tgtG4EObHSGzb1Nth8xOaicTUp0LUcHVUybufc0sRlmllTkJ6voEljN0vK+qqk05i64oVnWMcgWWI6gMyoBfmOMPm++BNsHhXQ+k9lTvEW7AcoQvSP3zp6LUi/KufTy6D3ATsRdJU2RcUVw9WSK1J55dLnB5I9Jy6D++L0q/VqlWMlpNozk2TJMJbE3eN3pbv9q50XrpBqukGYNj2IyhSXJ1gTqP3Y1j9VepO4Aj/RIp6RvsED/fPEfahmiZbXC0KtCwLSF1a8lrb1B30MSJuLFw7lz9rIlx48bt3LnTx9sbNPb33liKZoF1S/z1V2uYhjuQtcQi7pYZJqdkwErFV6msdHSUSMW6Xl3sU8Oxe2MksHxIYPmQwPIJB3uLQQLLhwSWDwksHxJqtRrLJxyc+5DA8iGB5UMC+6hEAuc+JLB8SODCiwTOfUhg+ZDA8iGB6z4kcO5DAsuHBNTOz88PiBix576CggIgYrCvIiSwfEiIWj5otWAflcLBuQ8JLB8SWD4ksHNtJHDuQwLLhwSWDwksHxJYPiSwfEhg+ZDA8iGBnWsLYdasWSkpKdzemvrlVfDg8uXLQGSIcVXzvHnzgoKCOM/aFEVxB9g/r7n06tUrOjq6YbGAT75RUVFAfIh0Tf306dMDAwP1b+Hx1KlTgfgQqXyRkZH9+vXjMiBN0127du3SpQsQH6J2rs15d/f19Z0yZQoQJeKVLyIiAmZAmPU6derUs2dPIEpawHA5lliYmapQVbP+tXXer4lmnrYZIZuaNz/J0Apyg5c27sib3TKZlBD2jpKAMPtB43yd3JA2pBEu3/2/VEcS86rK1SRFyhwkzl6OLh5Oju5ShtuUGHYSU/rPXLdOXL+gW++4nK4NZ5UhG6/4ZhqnAXULw4nGgQ1TNrhg02TcRwJ0jUKtKFUqSquVCpVGpbF3pKIHesYMF7IPBBAs39frs+VlKgcXu9CefpTsEfZ5l3OtSF6skEiJSUtDXT0trsoslu/OhcrfEgscnO3a9wsEbQUoYnmhvEOU88gZlk2psUy+G8mVJ/YVRMQEOrrbgTbH7RPZPsGy8QuDzD/FAvlSfis//2tRt9gw0Ha59ce9oAiHuLkBZqY3V74Lx8rPHS7qNjQMtHX+Ssr19pWOW2xW1WRuZXn20MNuQ8KADdBpQHBBbvXF38vMSWyWfJ+tzHT3dxGxid3CBD8WePZwsTkp+SX5c38xHOkPfswb2Awu3jKZk+S7TTm8Kfnlu3Wu3CNQoFX56NL+ieDiByreZDzy3ThdqVEzfh1FKp9cUZqw+okr138HLQ1JAak9dXDHA55kpqOvniq1cxDrBlJWxs3POS+rxnQaHvkqyzQeAc7AJvHv5KFWabVKU2lMjbRpq4FaqfUKE7IrpTlUVBYf/GVLVs41laqmc8e+QwfP9PUJheEPCtI3b52yaM6u46e+vpF60s3VN7rHsFHD5lO6XRYvXzt65Nin1dUVXSMHDu5v3S5ogiTOHC0ZMNrTWAJTuS/1ckULupJrAhy+2LHr5fSsS+NHr1i6INHZyfOjnTOLinNhlIRiF2L9sH9jz8dGvP1G0pQJa08mf3v1JlvBPShIS9y7JqbnqBWv/BgT/X/7f7aunxSSJAqyqk0lMBH38L5SmBcLc8jMvlJYlPXshLWRnfq5uniNHrkI9nb9eWaPPkFUtyFR3WMlEmn78F5eHkG592/DwNPnfnR38x/25IuOjq4dIh5/ImYssCakhKxWmJqfaarwalQ0SSL1Jpog695VipJ2jKh1YQcHc6FMGVn1I7nBgfWDG/b2LtU1rM/FopIcf78IfXi7oK7AmsDCy5h03WtKPomMsl7uq66Ra7VqaHY0DHR28tAfE4a8MlRVVXh71ftclMkcgFXh6yY3JZ+bp8x6gyEuzl7wy8+c2qjy4q1qYZlVq+uNCaVSAawJ69PR2dSKWFPyhXVzOn34IbAOQQGdVKpqd3c/b8/a6QPFJfcb5j6DeLgH3Lr9Jxw/4oS+dScJWBNGS3v5m+rZNPVre/pT0PguyzPV9AimY/vekR37/fC/t0rL8uWKsuRzez/c8fz5SwdNnxXVbSh80vjfz5thvkjLuHj63F5gTbRapts/TNltPDOsHF0kpXkV7oFWqWJmTnv/TMq+b75fdS/nuo93aK+okQP7TTJ9SueOTzw9YuGZ8/uWrekLm+Cpz6z95PM5wDoVdNG9SooifIJMPXTxdJcmHyi5llze5ckQYHvcPX3fzYOYuKSdiTQ8VXX/MZ6w/izPqwK2h6pKNfRZnpEj/umRwR0d8zJK3AIdjSVY9VaswXCNRgUtO4NOgPx9IhbM/gy0HF/sXpKZfdVglFqtlEoNV/8bVh4DRsi8WODsKvEM4OkuMWusY/tr6b7hXl6hLgZjS0rzDIbX1Mjt7Q13N5CkxN3NF7QcFRVFGq3h7jlFVYWTo+Hq39PD6IDGzd8zZ6/rIHUCpjFrcu5TE/yO/7fAmHwmPkSr4epqtDNcwMe782dOu85OvNoBM63iyN7OAREOd07xd163Ae5dzpdKwJjZZo1VmvtQEf9yoKevNPWPNuLixBh3T+epq1Qz14WZmd6yWQa/fFWYlapoq3bM3TN5FKmd+WaY+adYPMdl/468nL+qfMI9/Dq4g7aCokR172q+szM1Y41lOUPIDKvcNOXBz3IZhvAJc/cJf7QH4eTFyrzUh2qlJnqge/84L2Ahwuf3Hd1dkH5dTmsZmYPUxcfJK9RNavfIDKTDB7LKh/KaShXszvMPcxi3QKDxgDq79Nqpiqt/lsIRJa2G61uFZjIBe0R4TzRnwim0uFl3hLz+MAnd7ErAf0dQNz2VpEhHFyqih/OgeItzXKM7t+Cqoty7yrJCdY1CTfNdE34H2owvXO9YqCVgCGBnR3n4yEK7OIAW6kQX46KsRwixuzsROVg+JLB8SGD5kMDyIYHlQ+L/AQAA///w3J1bAAAABklEQVQDAAc/eV52rE82AAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce98deb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Understand the basic concept of load balancing: Load balancing is the process of distributing network or application traffic across multiple servers to ensure no single server becomes overwhelmed, thus improving responsiveness and availability.\n",
       "\n",
       "2. Learn about Nginx: Nginx is a high-performance web server that can also be used as a reverse proxy, load balancer, and HTTP cache.\n",
       "\n",
       "3. Install Nginx: Ensure Nginx is installed on your server. This can typically be done using package managers like apt for Ubuntu or yum for CentOS.\n",
       "\n",
       "4. Configure Nginx for load balancing: Open the Nginx configuration file, usually located at /etc/nginx/nginx.conf or /etc/nginx/conf.d/your-config-file.conf.\n",
       "\n",
       "5. Define the upstream servers: In the configuration file, define the upstream block where you list the backend servers that Nginx will distribute traffic to. For example: \n",
       " upstream backend {\n",
       "    server backend1.example.com;\n",
       "    server backend2.example.com;\n",
       " }\n",
       "\n",
       "6. Set up the server block: Configure the server block to use the upstream group. This is where you define the load balancing method (e.g., round-robin, least connections, IP hash). For example: \n",
       " server {\n",
       "    listen 80;\n",
       "    location / {\n",
       "        proxy_pass http://backend;\n",
       "    }\n",
       " }\n",
       "\n",
       "7. Choose a load balancing method: Nginx supports several load balancing methods such as round-robin (default), least connections, and IP hash. Choose the one that best fits your needs and configure it in the upstream block.\n",
       "\n",
       "8. Test the configuration: Before applying the changes, test the Nginx configuration for syntax errors using the command: nginx -t.\n",
       "\n",
       "9. Reload Nginx: If the configuration test is successful, reload Nginx to apply the changes using the command: nginx -s reload.\n",
       "\n",
       "10. Monitor and optimize: Continuously monitor the performance of your load balancer and backend servers. Adjust configurations as needed to optimize performance and handle traffic efficiently."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"To complete the final step, you need to set up a monitoring and optimization strategy for your Nginx load balancer and backend servers. Here's a detailed approach:\\n\\n1. **Monitoring Tools**: \\n   - Use tools like **Prometheus** and **Grafana** for real-time monitoring and visualization of server metrics.\\n   - Implement **Nginx Amplify** for monitoring Nginx performance metrics.\\n   - Consider using **ELK Stack (Elasticsearch, Logstash, Kibana)** for log management and analysis.\\n\\n2. **Metrics to Monitor**:\\n   - **Server Load**: Monitor CPU, memory, and disk usage on both the load balancer and backend servers.\\n   - **Network Traffic**: Keep an eye on incoming and outgoing traffic to identify any bottlenecks.\\n   - **Response Time**: Measure the time taken to serve requests to ensure it remains within acceptable limits.\\n   - **Error Rates**: Track HTTP error codes to identify issues with server responses.\\n\\n3. **Optimization Strategies**:\\n   - **Configuration Tuning**: Adjust Nginx configuration parameters such as worker processes, worker connections, and buffer sizes based on traffic patterns.\\n   - **Caching**: Implement caching strategies to reduce load on backend servers and improve response times.\\n   - **Load Balancing Method**: Re-evaluate the chosen load balancing method (round-robin, least connections, IP hash) and adjust if necessary based on traffic patterns.\\n   - **Scaling**: Consider horizontal scaling by adding more backend servers if traffic consistently exceeds current capacity.\\n\\n4. **Regular Audits**:\\n   - Conduct regular audits of your configuration and performance metrics to ensure optimal operation.\\n   - Stay updated with the latest Nginx releases and apply updates to benefit from performance improvements and security patches.\\n\\nBy implementing these monitoring and optimization strategies, you can ensure that your Nginx load balancer and backend servers operate efficiently, providing high availability and responsiveness to your users.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 398, 'prompt_tokens': 892, 'total_tokens': 1290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a4d13246c5', 'id': 'chatcmpl-Cktvhl9qUXJmPu1jLHcVo3Grm1IAR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--ffe89c35-0f9f-4374-b602-3182443b2186-0' usage_metadata={'input_tokens': 892, 'output_tokens': 398, 'total_tokens': 1290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response=app.invoke({\"input\":\"How Nginx works as a load balancer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0bbbb160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How Nginx works as a load balancer?',\n",
       " 'plan': ['Understand the basic concept of load balancing: Load balancing is the process of distributing network or application traffic across multiple servers to ensure no single server becomes overwhelmed, thus improving responsiveness and availability.',\n",
       "  'Learn about Nginx: Nginx is a high-performance web server that can also be used as a reverse proxy, load balancer, and HTTP cache.',\n",
       "  'Install Nginx: Ensure Nginx is installed on your server. This can typically be done using package managers like apt for Ubuntu or yum for CentOS.',\n",
       "  'Configure Nginx for load balancing: Open the Nginx configuration file, usually located at /etc/nginx/nginx.conf or /etc/nginx/conf.d/your-config-file.conf.',\n",
       "  'Define the upstream servers: In the configuration file, define the upstream block where you list the backend servers that Nginx will distribute traffic to. For example: \\n upstream backend {\\n    server backend1.example.com;\\n    server backend2.example.com;\\n }',\n",
       "  'Set up the server block: Configure the server block to use the upstream group. This is where you define the load balancing method (e.g., round-robin, least connections, IP hash). For example: \\n server {\\n    listen 80;\\n    location / {\\n        proxy_pass http://backend;\\n    }\\n }',\n",
       "  'Choose a load balancing method: Nginx supports several load balancing methods such as round-robin (default), least connections, and IP hash. Choose the one that best fits your needs and configure it in the upstream block.',\n",
       "  'Test the configuration: Before applying the changes, test the Nginx configuration for syntax errors using the command: nginx -t.',\n",
       "  'Reload Nginx: If the configuration test is successful, reload Nginx to apply the changes using the command: nginx -s reload.',\n",
       "  'Monitor and optimize: Continuously monitor the performance of your load balancer and backend servers. Adjust configurations as needed to optimize performance and handle traffic efficiently.'],\n",
       " 'past_steps': [],\n",
       " 'response': AIMessage(content=\"To complete the final step, you need to set up a monitoring and optimization strategy for your Nginx load balancer and backend servers. Here's a detailed approach:\\n\\n1. **Monitoring Tools**: \\n   - Use tools like **Prometheus** and **Grafana** for real-time monitoring and visualization of server metrics.\\n   - Implement **Nginx Amplify** for monitoring Nginx performance metrics.\\n   - Consider using **ELK Stack (Elasticsearch, Logstash, Kibana)** for log management and analysis.\\n\\n2. **Metrics to Monitor**:\\n   - **Server Load**: Monitor CPU, memory, and disk usage on both the load balancer and backend servers.\\n   - **Network Traffic**: Keep an eye on incoming and outgoing traffic to identify any bottlenecks.\\n   - **Response Time**: Measure the time taken to serve requests to ensure it remains within acceptable limits.\\n   - **Error Rates**: Track HTTP error codes to identify issues with server responses.\\n\\n3. **Optimization Strategies**:\\n   - **Configuration Tuning**: Adjust Nginx configuration parameters such as worker processes, worker connections, and buffer sizes based on traffic patterns.\\n   - **Caching**: Implement caching strategies to reduce load on backend servers and improve response times.\\n   - **Load Balancing Method**: Re-evaluate the chosen load balancing method (round-robin, least connections, IP hash) and adjust if necessary based on traffic patterns.\\n   - **Scaling**: Consider horizontal scaling by adding more backend servers if traffic consistently exceeds current capacity.\\n\\n4. **Regular Audits**:\\n   - Conduct regular audits of your configuration and performance metrics to ensure optimal operation.\\n   - Stay updated with the latest Nginx releases and apply updates to benefit from performance improvements and security patches.\\n\\nBy implementing these monitoring and optimization strategies, you can ensure that your Nginx load balancer and backend servers operate efficiently, providing high availability and responsiveness to your users.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 398, 'prompt_tokens': 892, 'total_tokens': 1290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a4d13246c5', 'id': 'chatcmpl-Cktvhl9qUXJmPu1jLHcVo3Grm1IAR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ffe89c35-0f9f-4374-b602-3182443b2186-0', usage_metadata={'input_tokens': 892, 'output_tokens': 398, 'total_tokens': 1290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7a1d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How Nginx works as a load balancer?\n"
     ]
    }
   ],
   "source": [
    "print(\"User Input:\",response['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0278d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan and Execution Steps: ['Understand the basic concept of load balancing: Load balancing is the process of distributing network or application traffic across multiple servers to ensure no single server becomes overwhelmed, thus improving responsiveness and availability.', 'Learn about Nginx: Nginx is a high-performance web server that can also be used as a reverse proxy, load balancer, and HTTP cache.', 'Install Nginx: Ensure Nginx is installed on your server. This can typically be done using package managers like apt for Ubuntu or yum for CentOS.', 'Configure Nginx for load balancing: Open the Nginx configuration file, usually located at /etc/nginx/nginx.conf or /etc/nginx/conf.d/your-config-file.conf.', 'Define the upstream servers: In the configuration file, define the upstream block where you list the backend servers that Nginx will distribute traffic to. For example: \\n upstream backend {\\n    server backend1.example.com;\\n    server backend2.example.com;\\n }', 'Set up the server block: Configure the server block to use the upstream group. This is where you define the load balancing method (e.g., round-robin, least connections, IP hash). For example: \\n server {\\n    listen 80;\\n    location / {\\n        proxy_pass http://backend;\\n    }\\n }', 'Choose a load balancing method: Nginx supports several load balancing methods such as round-robin (default), least connections, and IP hash. Choose the one that best fits your needs and configure it in the upstream block.', 'Test the configuration: Before applying the changes, test the Nginx configuration for syntax errors using the command: nginx -t.', 'Reload Nginx: If the configuration test is successful, reload Nginx to apply the changes using the command: nginx -s reload.', 'Monitor and optimize: Continuously monitor the performance of your load balancer and backend servers. Adjust configurations as needed to optimize performance and handle traffic efficiently.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Plan and Execution Steps:\", response['plan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6015e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To complete the final step, you need to set up a monitoring and optimization strategy for your Nginx load balancer and backend servers. Here's a detailed approach:\n",
       "\n",
       "1. **Monitoring Tools**: \n",
       "   - Use tools like **Prometheus** and **Grafana** for real-time monitoring and visualization of server metrics.\n",
       "   - Implement **Nginx Amplify** for monitoring Nginx performance metrics.\n",
       "   - Consider using **ELK Stack (Elasticsearch, Logstash, Kibana)** for log management and analysis.\n",
       "\n",
       "2. **Metrics to Monitor**:\n",
       "   - **Server Load**: Monitor CPU, memory, and disk usage on both the load balancer and backend servers.\n",
       "   - **Network Traffic**: Keep an eye on incoming and outgoing traffic to identify any bottlenecks.\n",
       "   - **Response Time**: Measure the time taken to serve requests to ensure it remains within acceptable limits.\n",
       "   - **Error Rates**: Track HTTP error codes to identify issues with server responses.\n",
       "\n",
       "3. **Optimization Strategies**:\n",
       "   - **Configuration Tuning**: Adjust Nginx configuration parameters such as worker processes, worker connections, and buffer sizes based on traffic patterns.\n",
       "   - **Caching**: Implement caching strategies to reduce load on backend servers and improve response times.\n",
       "   - **Load Balancing Method**: Re-evaluate the chosen load balancing method (round-robin, least connections, IP hash) and adjust if necessary based on traffic patterns.\n",
       "   - **Scaling**: Consider horizontal scaling by adding more backend servers if traffic consistently exceeds current capacity.\n",
       "\n",
       "4. **Regular Audits**:\n",
       "   - Conduct regular audits of your configuration and performance metrics to ensure optimal operation.\n",
       "   - Stay updated with the latest Nginx releases and apply updates to benefit from performance improvements and security patches.\n",
       "\n",
       "By implementing these monitoring and optimization strategies, you can ensure that your Nginx load balancer and backend servers operate efficiently, providing high availability and responsiveness to your users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Response:\")\n",
    "display(Markdown(response[\"response\"].content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6ec1db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Docker is a way to package, ship, and run software applications in containers.\\n\\nThink of it like shipping a box:\\n\\n* You have an application (like a website or a game) that you want to deploy on different computers.\\n* Instead of copying the entire operating system onto each computer, you can put just the application inside a special container called a \"docker container\".\\n* The container includes everything your application needs to run, like libraries and dependencies.\\n* When you send the container to another computer, it\\'s like sending a box with all the necessary stuff for your application.\\n\\nHere are some key benefits of Docker:\\n\\n1. **Lightweight**: Containers are much smaller than full operating systems, so they take up less space on your computer or server.\\n2. **Portable**: You can move containers from one computer to another without worrying about compatibility issues.\\n3. **Isolated**: Each container runs in its own isolated environment, so if something goes wrong with one application, it won\\'t affect other applications running in the same container.\\n\\nDocker also provides a way to manage and orchestrate these containers using tools like Docker Compose, which makes it easy to define and run multiple containers at once.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-12-09T16:07:03.702172967Z', 'done': True, 'done_reason': 'stop', 'total_duration': 31547457465, 'load_duration': 4307709836, 'prompt_eval_count': 32, 'prompt_eval_duration': 1751225569, 'eval_count': 241, 'eval_duration': 25148103964, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'} id='lc_run--3e312395-822f-4732-bae5-c7292fa6c0ad-0' usage_metadata={'input_tokens': 32, 'output_tokens': 241, 'total_tokens': 273}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain Docker in simple terms.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a2941ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker is a containerization platform that allows developers to package, ship, and run applications in containers. Containers are lightweight and portable, providing a consistent and reliable way to deploy applications across different environments.\n",
      "\n",
      "With Docker, you can:\n",
      "\n",
      "1. **Package your application**: You create a Docker image, which includes the application code, dependencies, and libraries needed to run it.\n",
      "2. **Ship your container**: The Docker image is packaged into a container, which can be easily shared with others or deployed to production.\n",
      "3. **Run your container**: The container is executed on a host machine or in a cloud environment, providing a consistent runtime environment for the application.\n",
      "\n",
      "Docker offers several benefits, including:\n",
      "\n",
      "1. **Lightweight and portable**: Containers are much smaller than virtual machines, making them easier to deploy and manage.\n",
      "2. **Isolated environments**: Containers provide a secure and isolated environment for applications, reducing the risk of conflicts between different applications.\n",
      "3. **Efficient resource usage**: Containers share the host operating system and only use resources necessary for the application, making them more efficient than virtual machines.\n",
      "4. **Easy deployment**: Docker makes it easy to deploy applications across different environments, such as development, testing, staging, and production.\n",
      "\n",
      "Some common terms related to Docker include:\n",
      "\n",
      "1. **Docker image**: A template that includes the application code, dependencies, and libraries needed to run it.\n",
      "2. **Docker container**: An instance of a running Docker image.\n",
      "3. **Docker Hub**: A cloud-based registry for storing and sharing Docker images.\n",
      "\n",
      "Some popular use cases for Docker include:\n",
      "\n",
      "1. **DevOps**: Docker enables teams to work together more efficiently by providing a consistent and reliable way to deploy applications.\n",
      "2. **Microservices architecture**: Docker makes it easy to deploy and manage microservices, which are independent components that make up a larger application.\n",
      "3. **Continuous integration and delivery (CI/CD)**: Docker integrates well with CI/CD tools, making it easy to automate the build, test, and deployment of applications.\n",
      "\n",
      "Overall, Docker is a powerful tool for simplifying the development, testing, and deployment of applications, while reducing costs and improving efficiency.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert DevOps engineer.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is Docker?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ea96ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a comprehensive overview of Docker basics:\n",
      "\n",
      "**What is Docker?**\n",
      "\n",
      "Docker is a containerization platform that allows developers to package, ship, and run applications in containers. Containers are lightweight and standalone, providing a consistent and reliable way to deploy applications.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "1. **Images**: A template for creating containers. Images contain the code, libraries, and settings needed to run an application.\n",
      "2. **Containers**: Running instances of images. Containers provide a isolated environment for running applications.\n",
      "3. **Docker Hub**: A registry of available Docker images that can be easily pulled and pushed.\n",
      "\n",
      "**Basic Docker Commands:**\n",
      "\n",
      "1. `docker run`: Runs a container from an image.\n",
      "Example: `docker run -it ubuntu /bin/bash`\n",
      "2. `docker ps`: Lists all running containers.\n",
      "3. `docker stop`: Stops a container.\n",
      "4. `docker rm`: Removes a stopped container.\n",
      "5. `docker images`: Lists all available Docker images on the system.\n",
      "6. `docker pull`: Pulls an image from Docker Hub.\n",
      "7. `docker push`: Pushes an image to Docker Hub.\n",
      "\n",
      "**Docker Networking:**\n",
      "\n",
      "1. **Container networking**: Allows containers to communicate with each other and with the host machine.\n",
      "2. **Bridge networks**: Creates a network for all containers that are connected to the same bridge.\n",
      "3. **Host networks**: Maps a container's network namespace to the host machine's network namespace.\n",
      "\n",
      "**Docker Volumes:**\n",
      "\n",
      "1. **Volume mounting**: Allows you to mount a host directory into a container.\n",
      "2. **Persistent data**: Ensures data is preserved even if the container is restarted or removed.\n",
      "\n",
      "**Docker Compose:**\n",
      "\n",
      "1. **Definition file**: A YAML file that defines services and their relationships.\n",
      "2. **Service definitions**: Define applications, networks, volumes, and ports.\n",
      "3. **Run command**: Specifies how to run each service.\n",
      "\n",
      "**Docker Security:**\n",
      "\n",
      "1. **User management**: Manage users and group memberships for Docker.\n",
      "2. **Authentication**: Authenticate users with Docker Hub or other authentication services.\n",
      "3. **Volume permissions**: Control access to data in volumes.\n",
      "\n",
      "This is just a starting point, and there's much more to explore in the world of Docker! Do you have any specific questions or areas you'd like me to expand on?"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Give me Docker basics.\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a249b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Docker Basics\n",
       "================\n",
       "\n",
       "### What is Docker?\n",
       "\n",
       "Docker is a containerization platform that allows developers to package, ship, and run applications in containers.\n",
       "\n",
       "### Key Concepts\n",
       "\n",
       "#### Containers\n",
       "\n",
       "*   A lightweight and portable way to deploy applications.\n",
       "*   A single process can be multiple containers.\n",
       "*   Can communicate with each other through API calls.\n",
       "\n",
       "#### Images\n",
       "\n",
       "*   The foundation of a container.\n",
       "*   A read-only template for an application.\n",
       "*   Includes the application code, libraries, and runtime environment.\n",
       "*   Can be shared among different environments.\n",
       "\n",
       "#### Docker Hub\n",
       "\n",
       "*   A central registry for Docker images.\n",
       "*   Allows users to push, pull, and manage images.\n",
       "*   Provides a convenient way to share images with others.\n",
       "\n",
       "### Installing Docker\n",
       "\n",
       "To install Docker on your system:\n",
       "\n",
       "1.  Go to the official [Docker website](https://www.docker.com/get-started) and download the installation package for your operating system.\n",
       "2.  Follow the instructions provided in the installation guide to complete the installation process.\n",
       "\n",
       "### Basic Docker Commands\n",
       "\n",
       "#### Running a Container\n",
       "\n",
       "```bash\n",
       "docker run -it --rm <image-name>\n",
       "```\n",
       "\n",
       "*   `-it`: Run the container interactively.\n",
       "*   `--rm`: Delete the container after it is closed.\n",
       "*   `<image-name>`: The name or ID of the image to use.\n",
       "\n",
       "#### Creating a New Image\n",
       "\n",
       "```bash\n",
       "docker build -t <image-name> .\n",
       "```\n",
       "\n",
       "*   `-t`: Specify the name and tag for the new image.\n",
       "*   `<image-name>`: The desired name for the new image.\n",
       "*   `.`: The directory containing the application code.\n",
       "\n",
       "#### Listing Images\n",
       "\n",
       "```bash\n",
       "docker images\n",
       "```\n",
       "\n",
       "Displays a list of all available images on your system.\n",
       "\n",
       "### Docker Network\n",
       "\n",
       "Docker networks allow containers to communicate with each other.\n",
       "\n",
       "*   Create a network:\n",
       "\n",
       "    ```bash\n",
       "docker network create <network-name>\n",
       "```\n",
       "\n",
       "*   Use the created network for a container:\n",
       "    ```bash\n",
       "docker run -it --net <network-name> <image-name>\n",
       "```\n",
       "*   Connect two containers using the network:\n",
       "\n",
       "    ```bash\n",
       "docker exec -it container1 docker connect container2 -n <network-name>\n",
       "```\n",
       "\n",
       "### Docker Compose\n",
       "\n",
       "Docker Compose is used to define and run multi-container Docker applications.\n",
       "\n",
       "*   Define a `docker-compose.yml` file:\n",
       "    ```yml\n",
       "version: '3'\n",
       "services:\n",
       "  web:\n",
       "    build: .\n",
       "    ports:\n",
       "      - \"8080:8080\"\n",
       "    depends_on:\n",
       "      - db\n",
       "  db:\n",
       "    image: postgres\n",
       "```\n",
       "\n",
       "*   Run the application:\n",
       "\n",
       "    ```\n",
       "docker-compose up\n",
       "```\n",
       "*   Stop the application:\n",
       "\n",
       "    ```bash\n",
       "docker-compose down\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "markdown_output = \"\"\n",
    "\n",
    "for chunk in llm.stream(\"Give me Docker basics in Markdown.\"):\n",
    "    markdown_output += chunk.content\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db67e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from langchain_ollama import ChatOllama\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def get_ollama_llm(model_name=\"llama3.2\"):\n",
    "    \"\"\"\n",
    "    Checks:\n",
    "    1. ollama installation\n",
    "    2. ollama model availability\n",
    "    Returns ChatOllama instance or raises helpful errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Check if ollama installed ---\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"--version\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            raise EnvironmentError(\"Ollama is installed? Command failed.\")\n",
    "\n",
    "        print(\" Ollama installed:\", result.stdout.strip())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise EnvironmentError(\n",
    "            \" Ollama is not installed on your system.\\n\"\n",
    "            \"Install it: https://ollama.com\"\n",
    "        )\n",
    "\n",
    "    # --- Check if model exists ---\n",
    "    try:\n",
    "        model_list = subprocess.run(\n",
    "            [\"ollama\", \"list\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        ).stdout\n",
    "        \n",
    "        if model_name not in model_list:\n",
    "            raise ValueError(\n",
    "                f\" Model '{model_name}' not found.\\n\"\n",
    "                f\"Install it using:  ollama pull {model_name}\"\n",
    "            )\n",
    "\n",
    "        print(f\" Model '{model_name}' found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error checking model: {str(e)}\")\n",
    "\n",
    "    # --- Return LangChain LLM instance ---\n",
    "    print(\" Loading model with LangChain...\")\n",
    "    return ChatOllama(model=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2a45a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama installed: ollama version is 0.13.2\n",
      " Model 'llama3.2' found.\n",
      " Loading model with LangChain...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.2')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ollama_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64854d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DockerBasicsResponse(BaseModel):\n",
    "    user_query: str = Field(..., description=\"The user's input question about Docker.\")\n",
    "    basics: str = Field(..., description=\"The basics of Docker explained in simple terms.\")\n",
    "    intermediate_steps: str = Field(..., description=\"Any intermediate steps or explanations.\")\n",
    "    advance_concepts: str = Field(..., description=\"Advanced concepts related to Docker.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77414dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama installed: ollama version is 0.13.2\n",
      " Model 'llama3.2' found.\n",
      " Loading model with LangChain...\n"
     ]
    }
   ],
   "source": [
    "llm = get_ollama_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7eba52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm.with_structured_output(DockerBasicsResponse).invoke(\"Explain Docker for Learning and DevOPs Engineer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f4526d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain docker for learning and devop engineer\n"
     ]
    }
   ],
   "source": [
    "print(response.user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b3bc9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker is an open-source platform for building, shipping, and running containerized applications. It's a lightweight and portable way to deploy software, making it easier to manage and scale applications.\n"
     ]
    }
   ],
   "source": [
    "print(response.basics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "88c5c77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are some key concepts and skills you'll need to know as a learning and DevOps engineer: \\n\\n1. Containerization: Understanding how Docker containers work and how they can be used to package and deploy applications.\\n2. Images and Volumes: Learning about Docker images, volumes, and how they're used to persist data in containers.\\n3. Networking and Ports: Understanding how Docker handles networking and port mappings.\\n4. Docker Compose: Using Docker Compose to define and run multi-container applications.\\n5. Orchestration: Understanding how Docker Swarm or Kubernetes can be used to orchestrate multiple containers across a cluster.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.intermediate_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "170c85ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Some advanced concepts in Docker include:\\n\\n1. Dockerfile: Creating custom Docker images using the Dockerfile syntax.\\n2. Docker Volumes: Using Docker volumes to persist data between container restarts.\\n3. Docker Network Mode: Understanding different network modes (bridge, host, etc.) and how they're used.\\n4. Docker Secret Management: Managing sensitive data using Docker secrets.\\n5. CI/CD Pipelines: Integrating Docker with continuous integration and delivery pipelines.\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.advance_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230d77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
